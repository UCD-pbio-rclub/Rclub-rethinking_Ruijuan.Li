---
title: "Chapter-06-part3-assignment"
author: "Ruijuan Li"
date: "May 21, 2016"
output:
  html_document:
    keep_md: yes
  pdf_document: default
---

# 6M2
```{r}
# Explain the difference between model selection and model averaging. What information is lost under model selection? What information is lost under model averaging? 

# model selection means choosing the model w/ the lowest AIC/DIC/WAIC value and the discarding the others
# model averaging means using DIC/WAIC to construct a posterior predictive distribution that exploit what we know about relative accuary of the models (don't understand thouroughly...)

# under model selection, the info about relative model accuracy contained in the differences among the AIC/DIC/WAIC values are discarded. (don't understand thouroughly...)

# under model averaging, we lost the 
```

# 6M3
```{r}
library(rethinking)
data(cars)
m <- map(
  alist(
    dist ~ dnorm(mu, sigma),
    mu <- a + b*speed,
    a ~ dnorm(0, 100),
    b ~ dnorm(0, 10),
    sigma ~ dunif(0, 30)
  ), data = cars)

DIC(m) # 419.5488
WAIC(m) # 420.6079

# with smaller number of prior 
cars.small <- cars[1:25,]
m <- map(
  alist(
    dist ~ dnorm(mu, sigma),
    mu <- a + b*speed,
    a ~ dnorm(0, 100),
    b ~ dnorm(0, 10),
    sigma ~ dunif(0, 30)
  ), data = cars.small)
DIC(m) # 205.9512
WAIC(m) # 210.4056
```

# 6M4
```{r}
# What happens to the effective number of parameters, as measured by DIC or WAIC, as a prior becomes more concentrated? Why? Perform some experiments, if you are not sure. 

# As a prior becomes more concentrated, which means priors becomes overly informative, under this case, there might be problem of underfitting, because the model learns too little from the training data. However, the same number of effective parameters should still be obtained by DIC or WAIC (see figure 6.9 left). 

data(cars)
m.cocentrated <- map(
  alist(
    dist ~ dnorm(mu, sigma),
    mu <- a + b*speed,
    a ~ dnorm(0, 10),
    b ~ dnorm(0, 1),
    sigma ~ dunif(0, 30)
  ), data = cars)

DIC(m) # 206
WAIC(m) # 212

# almost no change in DIC & WAIC values, so --> not much change in effective number of parameters...
```

# hard 
```{r}
# load data
data(Howell1)
d <- Howell1
head(d)
dim(d) # 544
d$age <- (d$age - mean(d$age))/sd(d$age)
set.seed(1000)
i <- sample(1:nrow(d), size=nrow(d)/2)
i
length(i) # 272 
?sample # takes a sample of the specified size from the elements of x using either w/ or w/ repleacement
d1 <- d[i, ]
head(d1) 
dim(d1)
d2 <- d[-i, ] # split d randomly into d1 and d2 

# fit models w/ different number of parameters
height.start <- mean(d1$height) 
sigma.start <- log(sd(d1$height))

m6H1.1 <- map(
 alist(
  height ~ dnorm(mu, exp(log.sigma)), 
  mu <- a + b1*age # understand a here 
 ), data = d1, start = list(a=height.start, b1=0, log.sigma = sigma.start)) 

m6H1.2 <- map(
 alist(
  height ~ dnorm(mu, exp(log.sigma)), 
  mu <- a + b1*age + b2*age^2
 ), data = d1, start = list(a=height.start, b1=0, b2=0, log.sigma = sigma.start)) 

m6H1.3 <- map(
 alist(
  height ~ dnorm(mu, exp(log.sigma)), 
  mu <- a + b1*age + b2*age^2 + b3*age^3
 ), data = d1, start = list(a=height.start, b1=0, b2=0, b3=0, log.sigma = sigma.start)) 

m6H1.4 <- map(
 alist(
  height ~ dnorm(mu, exp(log.sigma)), 
  mu <- a + b1*age + b2*age^2 + b3*age^3 + b4*age^4
 ), data = d1, start = list(a=height.start, b1=0, b2=0, b3=0, b4=0, log.sigma = sigma.start)) 

m6H1.5 <- map(
 alist(
  height ~ dnorm(mu, exp(log.sigma)), 
  mu <- a + b1*age + b2*age^2 + b3*age^3 + b4*age^4 + b5*age^5
 ), data = d1, start = list(a=height.start, b1=0, b2=0, b3=0, b4=0, b5=0, log.sigma = sigma.start)) 

m6H1.6 <- map(
 alist(
  height ~ dnorm(mu, exp(log.sigma)), 
  mu <- a + b1*age + b2*age^2 + b3*age^3 + b4*age^4 + b5*age^5 + b6*age^6
 ), data = d1, start = list(a=height.start, b1=0, b2=0, b3=0, b4=0, b5=0, b6=0, log.sigma = sigma.start)) 
```

# 6H1
```{r}
compare(m6H1.1, m6H1.2, m6H1.3, m6H1.4, m6H1.5, m6H1.6)
# m6H1.4 is the best model based on the model ranking and WAIC weights
# A model's weight is an estimate of the probability that the model will make the best predictions on new data, conditional on the set of models considered. 
```

# 6H2
```{r, results='hide'}
# For each model, produce a plot with model averaged mean and 97% confidence interval of the mean, superimposed on the raw data. How do predictions differ across models? (refer to chapter 4.4 P92)
age.seq <- seq(-2, 3, by = 0.1) # get age sample 
for(model in ls(pattern="^m6H1.[1-6]$")) { # get from Julin... 
  mu <- link(get(model), data = data.frame(age=age.seq)) # compute height mean/model average mean for different age using model
  dim(mu)
  mu.mean <- apply(mu, 2, mean)
  mu
  mu.HPDI <- apply(mu, 2, HPDI, prob=0.97)
  mu.HPDI
  plot(d1$age, d1$height, xlim=c(-2, 3), ylim=range(d$height)) # the raw data
  lines(age.seq, mu.mean) # the mean mu for each age
  shade(mu.HPDI, age.seq) # the shaded region for 97% HPDI 
}
?get # get the content of the specified object  
?assign # assign a value to a name 
# for markdown, adding results=hide help you hide the text but only show you the plot. (chunk options) 
``` 

# 6H3
```{r, results='hide'}
# Now also plot the model averaged predictions, across all models. In what ways do the averaged predictions differ from the predictions of the model with the lowest WAIC value? (about model averaging, chapter 6.5, R code 6.29 & 6.30)
# the best model 
age.seq <- seq(-2, 3, length.out = 272)
d.predict <- list(
  height = rep(0, 272),
  age = age.seq
)

# model averaging 
height.ensemble <- ensemble(m6H1.1, m6H1.2, m6H1.3, m6H1.4, m6H1.5, m6H1.6, data = d.predict) # why d.predict here??? 
mu.ensemble.mean <- apply(height.ensemble$link, 2, mean)
mu.ensemble.HPDI <- apply(height.ensemble$link, 2, HPDI, prob=0.97)
plot(d1$age, d1$height)
lines(d.predict$age, mu.ensemble.mean) 
shade(mu.ensemble.HPDI, d.predict$age)  
```

# 6H4
```{r}
# Compute the test-sample deviance for each model. (refer to R code 6.11)
# for(model in ls(pattern="^m6H1.[1-6]$")) {  
#  theta <- coef(get(model)) # extract MAP estimates 
  # below calcualte the deviance for test-sample 
#  dev <- (-2)*sum(dnorm(d2$height, 
#                        mean = theta[1]+theta[2]*d2$age,
#                        sd = theta[3],
#                        log = TRUE))
#  print(dev)
#}
#head(d2)

# the above is refering to R code 6.11, however it doesn't work; below is from Julin, it dosn't work for me either... 

# models <- ls(pattern="^m6H1.[1-6]$")
# test.dev <- sapply(models,function(m) {
#   model <- get(m)
#   input <- as.list(coef(model))
#   input$age <- d2$age
#   equation <- model@links[[1]][[2]] 
#   mu <- eval(parse(text=equation),envir = input)
#   dev <- -2*sum(dnorm(d2$height,mu,input$sigma,log=T))
#   dev
# })
# test.dev

```

# 6H5
```{r}
# Based on Julin's test, both test-deviance & WAIC give the smallest value for m6H1.4, so WAIC does a good job of estimating the test deviance. 
```

# 6H6
```{r}
# fit the model using stronger regularizing priors 
m6H1.7 <- map(
 alist(
  height ~ dnorm(mu, exp(log.sigma)), 
  mu <- a + b1*age + b2*age^2 + b3*age^3 + b4*age^4 + b5*age^5 + b6*age^6, 
  b1 ~ dnorm(0, 5),
  b2 ~ dnorm(0, 5),
  b3 ~ dnorm(0, 5),
  b4 ~ dnorm(0, 5),
  b5 ~ dnorm(0, 5),
  b6 ~ dnorm(0, 5)
 ), data = d1, start = list(a=height.start, log.sigma = sigma.start)) 

# report the MAP estimates 
WAIC(m6H1.7) # 

# plot the implied predictions 
age.seq <- seq(-2, 3, by = 0.1) # get age sample 

mu <- link(m6H1.7, data = data.frame(age=age.seq)) # compute height mean/model average mean for different age using model
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob=0.97)
plot(d1$age, d1$height, xlim=c(-2, 3), ylim=range(d$height)) # the raw data
lines(age.seq, mu.mean) # the mean mu for each age
shade(mu.HPDI, age.seq) # the shaded region for 97% HPDI 

# compute out-of-sample deviance using the data in d2
theta <- coef(m6H1.7) # extract MAP estimates 
dev <- (-2)*sum(dnorm(d2$height, 
                mean = theta[1]+theta[2]*d2$age,
                sd = theta[3],
                log = TRUE))
dev # 9327.714 # not sure whether this is right... 

# 
compare(m6H1.6, m6H1.7) # based on this compare result, the 7th model looks better... 
# I still didn't get the out-of-sample deviance calculated correctly... 
```



